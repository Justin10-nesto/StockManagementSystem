{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CTS\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:173: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.19.2)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from textblob import TextBlob\n",
    "from PyPDF2 import PdfFileReader\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\CTS\\Downloads\\ELearning\\Qns Generation\\test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "raw_doc =\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pdf = PdfFileReader(path)\n",
    "# for page in pdf.pages:\n",
    "#     page = pdf.getPage(1)\n",
    "#     raw_doc += page.extract_text()\n",
    "# page = pdf.pages[0]\n",
    "# print(page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path,'rb') as f:\n",
    "    for line in f:\n",
    "        line = str(line.strip())\n",
    "        raw_doc +=line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw_doc.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = nltk.word_tokenize(raw_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = ['refer', 'define', 'process', 'is the', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find synonyms\n",
    "def similar_words(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.name() != word:\n",
    "                synonyms.append(lemma.name())\n",
    "    return set(synonyms)\n",
    "            \n",
    "# Find antonyms\n",
    "def opposite_words(word):\n",
    "    antonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                antonyms.append(antonym.name())\n",
    "    return aset(antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions_modified = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtn in definitions:\n",
    "    similar = similar_words(dtn)\n",
    "    for sim in similar:\n",
    "        definitions_modified.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mention',\n",
       " 'relate',\n",
       " 'have-to_doe_with',\n",
       " 'pertain',\n",
       " 'bring_up',\n",
       " 'cite',\n",
       " 'look_up',\n",
       " 'denote',\n",
       " 'touch_on',\n",
       " 'come_to',\n",
       " 'advert',\n",
       " 'bear_on',\n",
       " 'concern',\n",
       " 'name',\n",
       " 'touch',\n",
       " 'consult',\n",
       " 'fix',\n",
       " 'delineate',\n",
       " 'delimitate',\n",
       " 'limit',\n",
       " 'determine',\n",
       " 'specify',\n",
       " 'set',\n",
       " 'delimit',\n",
       " 'outgrowth',\n",
       " 'summons',\n",
       " 'mental_process',\n",
       " 'swear_out',\n",
       " 'operation',\n",
       " 'action',\n",
       " 'serve',\n",
       " 'cognitive_operation',\n",
       " 'cognitive_process',\n",
       " 'litigate',\n",
       " 'march',\n",
       " 'physical_process',\n",
       " 'unconscious_process',\n",
       " 'appendage',\n",
       " 'treat',\n",
       " 'procedure',\n",
       " 'sue',\n",
       " 'work_on',\n",
       " 'work']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definitions_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # using punkt tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# from gensim import  corpora\n",
    "# from gensim.models.idamodel import LdaModel\n",
    "# from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating ditionary of words\n",
    "# dictionary = corpora.Dictionary(new_data)\n",
    "\n",
    "# # creating a document-term matrix\n",
    "# corpus = [dictionary.doc2bow(doc) for doc in new_data]\n",
    "\n",
    "# # train LDA Model\n",
    "# lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topic =2, passes = 10)\n",
    "\n",
    "# # interpret the results\n",
    "# pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing\n",
    "lemmer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalizer(text):\n",
    "    return lemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robol_response = ''\n",
    "    tifidvec = TfidfVectorizer(tokenizer=LemNormalizer, stop_words='english')\n",
    "    tfid = tifidvec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfid[-1], tfid)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tif = flat[-2]\n",
    "    if (req_tif == 0):\n",
    "        robol_response = robol_response + \"I am sorry, i can't understand you\"\n",
    "        return robol_response\n",
    "    else:\n",
    "        robol_response = robol_response + sent_tokens[idx]\n",
    "        sent_tokens.remove(ent_tokens[idx])\n",
    "        return robol_response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 11001] getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = TextBlob(raw_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = text.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_words = len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict = {'word':[], 'tag':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_word, previous_tag = 'ml', 'NN'\n",
    "sentence_words = ''\n",
    "for i in range(no_words+1):\n",
    "    status = False\n",
    "    try:\n",
    "        tag_word = tags[i][1]\n",
    "    #         print(tags[i],tags[i+1]\n",
    "        if tag_word == tags[i-1][1]:\n",
    "            sentence_words += tags[i-1][0]+' '\n",
    "        else:\n",
    "            sentence_words+=tags[i-1][0]\n",
    "            status =True\n",
    "\n",
    "        if status:\n",
    "            words_dict['word'].append(sentence_words)\n",
    "            words_dict['tag'].append(tags[i-1][1])\n",
    "            sentence_words = ''\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': [\"'\",\n",
       "  \"b'Python\",\n",
       "  'Machine Learning',\n",
       "  \"'\",\n",
       "  \"b'Unlock deeper\",\n",
       "  'insights',\n",
       "  'into',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'with',\n",
       "  'this',\n",
       "  \"'\",\n",
       "  \"b'vital\",\n",
       "  'guide',\n",
       "  'to',\n",
       "  'cutting-edge predictive',\n",
       "  'analytics',\n",
       "  \"'\",\n",
       "  \"b'Sebastian\",\n",
       "  'Raschka',\n",
       "  \"'\",\n",
       "  \"b'BIRMINGHAM\",\n",
       "  'MUMBAI',\n",
       "  \"'\",\n",
       "  \"b'Python\",\n",
       "  'Machine Learning',\n",
       "  \"'\",\n",
       "  \"b'Copyright \\\\xc2\\\\xa9\",\n",
       "  '2016',\n",
       "  'Packt Publishing',\n",
       "  \"'\",\n",
       "  \"b'All\",\n",
       "  'rights',\n",
       "  'reserved',\n",
       "  'No',\n",
       "  'part',\n",
       "  'of',\n",
       "  'this',\n",
       "  'book',\n",
       "  'may',\n",
       "  'be',\n",
       "  'reproduced stored',\n",
       "  'in',\n",
       "  'a',\n",
       "  \"retrieval b'system\",\n",
       "  'or',\n",
       "  'transmitted',\n",
       "  'in',\n",
       "  'any',\n",
       "  'form',\n",
       "  'or',\n",
       "  'by',\n",
       "  'any',\n",
       "  'means',\n",
       "  'without',\n",
       "  'the',\n",
       "  'prior',\n",
       "  'written',\n",
       "  \"'\",\n",
       "  \"b'permission\",\n",
       "  'of',\n",
       "  'the',\n",
       "  'publisher',\n",
       "  'except in',\n",
       "  'the',\n",
       "  'case',\n",
       "  'of',\n",
       "  'brief',\n",
       "  'quotations',\n",
       "  'embedded',\n",
       "  'in',\n",
       "  \"b'critical\",\n",
       "  'articles',\n",
       "  'or',\n",
       "  'reviews',\n",
       "  \"'\",\n",
       "  \"b'Every effort\",\n",
       "  'has',\n",
       "  'been made',\n",
       "  'in',\n",
       "  'the',\n",
       "  'preparation',\n",
       "  'of',\n",
       "  'this',\n",
       "  'book',\n",
       "  'to',\n",
       "  'ensure',\n",
       "  'the',\n",
       "  'accuracy',\n",
       "  \"'\",\n",
       "  \"b'of\",\n",
       "  'the',\n",
       "  'information',\n",
       "  'presented',\n",
       "  'However',\n",
       "  'the',\n",
       "  'information',\n",
       "  'contained',\n",
       "  'in',\n",
       "  'this',\n",
       "  'book',\n",
       "  'is',\n",
       "  \"'\",\n",
       "  \"b'sold\",\n",
       "  'without',\n",
       "  'warranty',\n",
       "  'either',\n",
       "  'express',\n",
       "  'or',\n",
       "  'implied',\n",
       "  'Neither',\n",
       "  'the',\n",
       "  'author',\n",
       "  'nor',\n",
       "  'Packt',\n",
       "  \"'\",\n",
       "  \"b'Publishing\",\n",
       "  'and',\n",
       "  'its',\n",
       "  'dealers',\n",
       "  'and',\n",
       "  'distributors',\n",
       "  'will',\n",
       "  'be',\n",
       "  'held',\n",
       "  'liable',\n",
       "  'for',\n",
       "  'any',\n",
       "  'damages',\n",
       "  \"'\",\n",
       "  \"b'caused\",\n",
       "  'or',\n",
       "  'alleged',\n",
       "  'to',\n",
       "  'be',\n",
       "  'caused',\n",
       "  'directly',\n",
       "  'or',\n",
       "  'indirectly',\n",
       "  'by',\n",
       "  'this',\n",
       "  'book',\n",
       "  \"'\",\n",
       "  \"b'Packt\",\n",
       "  'Publishing',\n",
       "  'has',\n",
       "  'endeavored',\n",
       "  'to',\n",
       "  'provide',\n",
       "  'trademark information',\n",
       "  'about',\n",
       "  'all',\n",
       "  'of',\n",
       "  'the',\n",
       "  \"'\",\n",
       "  \"b'companies\",\n",
       "  'and',\n",
       "  'products',\n",
       "  'mentioned',\n",
       "  'in',\n",
       "  'this',\n",
       "  'book',\n",
       "  'by',\n",
       "  'the',\n",
       "  'appropriate',\n",
       "  'use',\n",
       "  'of',\n",
       "  'capitals',\n",
       "  \"b'However\",\n",
       "  'Packt Publishing',\n",
       "  'can',\n",
       "  'not',\n",
       "  'guarantee',\n",
       "  'the',\n",
       "  'accuracy',\n",
       "  'of',\n",
       "  'this',\n",
       "  'information',\n",
       "  \"'\",\n",
       "  \"b'First\",\n",
       "  'published',\n",
       "  'September',\n",
       "  '2015',\n",
       "  \"'\",\n",
       "  \"b'Production reference\",\n",
       "  '3150416',\n",
       "  \"b'Published\",\n",
       "  'by',\n",
       "  'Packt Publishing Ltd.',\n",
       "  \"'\",\n",
       "  \"b'Livery\",\n",
       "  'Place',\n",
       "  \"'\",\n",
       "  \"b'35\",\n",
       "  'Livery Street',\n",
       "  \"'\",\n",
       "  \"b'Birmingham\",\n",
       "  'B3',\n",
       "  '2PB',\n",
       "  'UK',\n",
       "  \"b'ISBN\",\n",
       "  '978-1-78355-513-0',\n",
       "  \"b'www.packtpub.com b\",\n",
       "  \"b'Credits\",\n",
       "  \"'\",\n",
       "  \"b'Author\",\n",
       "  \"b'Sebastian\",\n",
       "  'Raschka',\n",
       "  \"'\",\n",
       "  \"b'Reviewers\",\n",
       "  \"'\",\n",
       "  \"b'Richard\",\n",
       "  'Dutton',\n",
       "  \"'\",\n",
       "  \"b'Dave\",\n",
       "  'Julian',\n",
       "  \"'\",\n",
       "  \"b'Vahid\",\n",
       "  'Mirjalili',\n",
       "  \"'\",\n",
       "  \"b'Hamidreza\",\n",
       "  'Sattari',\n",
       "  \"'\",\n",
       "  \"b'Dmytro\",\n",
       "  'Taranovsky',\n",
       "  \"'\",\n",
       "  \"b'Commissioning\",\n",
       "  'Editor',\n",
       "  \"'\",\n",
       "  \"b'Akram\",\n",
       "  'Hussain',\n",
       "  \"'\",\n",
       "  \"b'Acquisition\",\n",
       "  'Editors',\n",
       "  \"'\",\n",
       "  \"b'Rebecca\",\n",
       "  'Youe',\n",
       "  \"'\",\n",
       "  \"b'Meeta\",\n",
       "  'Rajani',\n",
       "  \"'\",\n",
       "  \"b'Content\",\n",
       "  'Development Editor',\n",
       "  \"'\",\n",
       "  \"b'Riddhi\",\n",
       "  'Tuljapurkar',\n",
       "  \"'\",\n",
       "  \"b'Technical\",\n",
       "  'Editors',\n",
       "  \"'\",\n",
       "  \"b'Madhunikita\",\n",
       "  'Sunil Chindarkar',\n",
       "  \"'\",\n",
       "  \"b'Taabish\",\n",
       "  'Khan',\n",
       "  \"'\",\n",
       "  \"b'Copy\",\n",
       "  'Editors',\n",
       "  \"'\",\n",
       "  \"b'Roshni\",\n",
       "  'Banerjee',\n",
       "  \"'\",\n",
       "  \"b'Stephan\",\n",
       "  'Copestake',\n",
       "  \"'\",\n",
       "  \"b'Project\",\n",
       "  'Coordinator',\n",
       "  \"'\",\n",
       "  \"b'Kinjal\",\n",
       "  'Bari',\n",
       "  \"'\",\n",
       "  \"b'Proofreader\",\n",
       "  \"b'Safis\",\n",
       "  'Editing',\n",
       "  \"'\",\n",
       "  \"b'Indexer b'Hemangini\",\n",
       "  'Bari',\n",
       "  \"'\",\n",
       "  \"b'Graphics\",\n",
       "  \"'\",\n",
       "  \"b'Sheetal\",\n",
       "  'Aute',\n",
       "  \"'\",\n",
       "  \"b'Abhinash\",\n",
       "  'Sahu',\n",
       "  \"'\",\n",
       "  \"b'Production\",\n",
       "  'Coordinator',\n",
       "  \"'\",\n",
       "  \"b'Shantanu\",\n",
       "  'N. Zagade',\n",
       "  \"'\",\n",
       "  \"b'Cover\",\n",
       "  'Work',\n",
       "  \"'\",\n",
       "  \"b'Shantanu\",\n",
       "  'N. Zagade',\n",
       "  \"'\",\n",
       "  \"b b'Foreword\",\n",
       "  \"b'We\",\n",
       "  'live',\n",
       "  'in',\n",
       "  'the',\n",
       "  'midst',\n",
       "  'of',\n",
       "  'a',\n",
       "  'data deluge',\n",
       "  'According',\n",
       "  'to',\n",
       "  'recent',\n",
       "  'estimates',\n",
       "  '2.5',\n",
       "  'quintillion b',\n",
       "  '1018',\n",
       "  'bytes',\n",
       "  'of',\n",
       "  'data',\n",
       "  'are',\n",
       "  'generated',\n",
       "  'on',\n",
       "  'a',\n",
       "  'daily',\n",
       "  'basis',\n",
       "  'This',\n",
       "  'is',\n",
       "  'so',\n",
       "  'much',\n",
       "  'data',\n",
       "  'that over',\n",
       "  '90',\n",
       "  \"'\",\n",
       "  \"b'percent\",\n",
       "  'of',\n",
       "  'the',\n",
       "  'information',\n",
       "  'that',\n",
       "  'we',\n",
       "  'store',\n",
       "  'nowadays',\n",
       "  'was',\n",
       "  'generated',\n",
       "  'in',\n",
       "  'the',\n",
       "  'past',\n",
       "  \"decade b'alone\",\n",
       "  'Unfortunately',\n",
       "  'most',\n",
       "  'of',\n",
       "  'this',\n",
       "  'information',\n",
       "  'can',\n",
       "  'not',\n",
       "  'be',\n",
       "  'used',\n",
       "  'by',\n",
       "  'humans',\n",
       "  'Either',\n",
       "  'the',\n",
       "  \"b'data\",\n",
       "  'is',\n",
       "  'beyond',\n",
       "  'the',\n",
       "  'means',\n",
       "  'of',\n",
       "  'standard analytical',\n",
       "  'methods',\n",
       "  'or',\n",
       "  'it',\n",
       "  'is',\n",
       "  'simply too',\n",
       "  'vast',\n",
       "  'for',\n",
       "  \"'\",\n",
       "  \"b'our\",\n",
       "  'limited',\n",
       "  'minds',\n",
       "  'to',\n",
       "  'even',\n",
       "  'comprehend',\n",
       "  \"'\",\n",
       "  \"b'Through\",\n",
       "  'Machine Learning',\n",
       "  'we',\n",
       "  'enable',\n",
       "  'computers',\n",
       "  'to',\n",
       "  'process',\n",
       "  'learn',\n",
       "  'from',\n",
       "  'and',\n",
       "  'draw',\n",
       "  \"b'actionable\",\n",
       "  'insights',\n",
       "  'out of',\n",
       "  'the',\n",
       "  'otherwise',\n",
       "  'impenetrable',\n",
       "  'walls',\n",
       "  'of',\n",
       "  'big',\n",
       "  'data',\n",
       "  'From',\n",
       "  'the',\n",
       "  \"'\",\n",
       "  'b',\n",
       "  'massive',\n",
       "  'supercomputers',\n",
       "  'that',\n",
       "  'support',\n",
       "  'Google',\n",
       "  \"'s\",\n",
       "  'search',\n",
       "  'engines',\n",
       "  'to',\n",
       "  'the',\n",
       "  'smartphones',\n",
       "  \"b'that\",\n",
       "  'we',\n",
       "  'carry',\n",
       "  'in',\n",
       "  'our',\n",
       "  'pockets',\n",
       "  'we',\n",
       "  'rely',\n",
       "  'on',\n",
       "  'Machine Learning',\n",
       "  'to',\n",
       "  'power',\n",
       "  'most',\n",
       "  'of',\n",
       "  'the',\n",
       "  \"'\",\n",
       "  \"b'world\",\n",
       "  'around',\n",
       "  'us\\\\xe2\\\\x80\\\\x94often',\n",
       "  'without',\n",
       "  'even',\n",
       "  'knowing',\n",
       "  'it',\n",
       "  \"'\",\n",
       "  \"b'As\",\n",
       "  'modern',\n",
       "  'pioneers',\n",
       "  'in',\n",
       "  'the',\n",
       "  'brave new',\n",
       "  'world',\n",
       "  'of',\n",
       "  'big',\n",
       "  'data',\n",
       "  'it',\n",
       "  'then',\n",
       "  'behooves',\n",
       "  'us',\n",
       "  'to',\n",
       "  'learn',\n",
       "  \"b'more\",\n",
       "  'about',\n",
       "  'Machine Learning',\n",
       "  'What',\n",
       "  'is',\n",
       "  'Machine Learning',\n",
       "  'and',\n",
       "  'how',\n",
       "  'does',\n",
       "  'it',\n",
       "  'work',\n",
       "  \"'\",\n",
       "  \"b'How\",\n",
       "  'can',\n",
       "  'I',\n",
       "  'use',\n",
       "  'Machine',\n",
       "  'Learning',\n",
       "  'to',\n",
       "  'take',\n",
       "  'a',\n",
       "  'glimpse',\n",
       "  'into',\n",
       "  'the',\n",
       "  'unknown',\n",
       "  'power',\n",
       "  'my',\n",
       "  \"b'business\",\n",
       "  'or',\n",
       "  'just',\n",
       "  'find',\n",
       "  'out',\n",
       "  'what',\n",
       "  'the',\n",
       "  'Internet',\n",
       "  'at',\n",
       "  'large',\n",
       "  'thinks',\n",
       "  'about',\n",
       "  'my',\n",
       "  'favorite',\n",
       "  'movie',\n",
       "  \"'\",\n",
       "  \"b'All\",\n",
       "  'of',\n",
       "  'this',\n",
       "  'and',\n",
       "  'more',\n",
       "  'will',\n",
       "  'be',\n",
       "  'covered',\n",
       "  'in',\n",
       "  'the',\n",
       "  'following',\n",
       "  'chapters',\n",
       "  'authored',\n",
       "  'by',\n",
       "  'my',\n",
       "  'good',\n",
       "  \"'\",\n",
       "  \"b'friend\",\n",
       "  'and',\n",
       "  'colleague',\n",
       "  'Sebastian',\n",
       "  'Raschka',\n",
       "  \"'\",\n",
       "  \"b'When\",\n",
       "  'away',\n",
       "  'from',\n",
       "  'taming',\n",
       "  'my',\n",
       "  'otherwise',\n",
       "  'irascible',\n",
       "  'pet dog',\n",
       "  'Sebastian',\n",
       "  'has',\n",
       "  'tirelessly',\n",
       "  \"b'devoted\",\n",
       "  'his',\n",
       "  'free',\n",
       "  'time',\n",
       "  'to',\n",
       "  'the',\n",
       "  'open',\n",
       "  'source',\n",
       "  'Machine Learning',\n",
       "  'community',\n",
       "  'Over',\n",
       "  'the',\n",
       "  \"'\",\n",
       "  \"b'past several\",\n",
       "  'years',\n",
       "  'Sebastian',\n",
       "  'has',\n",
       "  'developed',\n",
       "  'dozens',\n",
       "  'of',\n",
       "  'popular',\n",
       "  'tutorials',\n",
       "  'that',\n",
       "  'cover',\n",
       "  \"b'topics\",\n",
       "  'in',\n",
       "  'Machine Learning',\n",
       "  'and',\n",
       "  'data',\n",
       "  'visualization',\n",
       "  'in',\n",
       "  'Python',\n",
       "  'He',\n",
       "  'has',\n",
       "  'also',\n",
       "  'developed',\n",
       "  \"b'and\",\n",
       "  'contributed',\n",
       "  'to',\n",
       "  'several open',\n",
       "  'source',\n",
       "  'Python',\n",
       "  'packages',\n",
       "  'several',\n",
       "  'of',\n",
       "  'which',\n",
       "  'are',\n",
       "  'now',\n",
       "  \"'\",\n",
       "  \"b'part\",\n",
       "  'of',\n",
       "  'the',\n",
       "  'core',\n",
       "  'Python Machine Learning',\n",
       "  'workflow b',\n",
       "  'Owing',\n",
       "  'to',\n",
       "  'his',\n",
       "  'vast',\n",
       "  'expertise',\n",
       "  'in',\n",
       "  'this',\n",
       "  'field',\n",
       "  'I',\n",
       "  'am',\n",
       "  'confident',\n",
       "  'that',\n",
       "  'Sebastian',\n",
       "  \"'s\",\n",
       "  'insights',\n",
       "  'into',\n",
       "  \"b'the\",\n",
       "  'world',\n",
       "  'of',\n",
       "  'Machine Learning',\n",
       "  'in',\n",
       "  'Python',\n",
       "  'will',\n",
       "  'be',\n",
       "  'invaluable',\n",
       "  'to',\n",
       "  'users',\n",
       "  'of',\n",
       "  'all',\n",
       "  'experience',\n",
       "  \"b'levels\",\n",
       "  'I',\n",
       "  'wholeheartedly',\n",
       "  'recommend',\n",
       "  'this',\n",
       "  'book',\n",
       "  'to',\n",
       "  'anyone',\n",
       "  'looking',\n",
       "  'to',\n",
       "  'gain',\n",
       "  'a',\n",
       "  'broader',\n",
       "  \"'\",\n",
       "  \"b'and\",\n",
       "  'more',\n",
       "  'practical',\n",
       "  'understanding',\n",
       "  'of',\n",
       "  'Machine Learning',\n",
       "  \"b'Dr\",\n",
       "  'Randal S. Olson',\n",
       "  \"'\",\n",
       "  \"b'Artificial\",\n",
       "  'Intelligence',\n",
       "  'and',\n",
       "  'Machine Learning Researcher University',\n",
       "  'of',\n",
       "  'Pennsylvania',\n",
       "  \"'\",\n",
       "  \"b'About\",\n",
       "  'the',\n",
       "  'Author',\n",
       "  \"'\",\n",
       "  \"b'Sebastian\",\n",
       "  'Raschka',\n",
       "  'is',\n",
       "  'a',\n",
       "  'PhD',\n",
       "  'student',\n",
       "  'at',\n",
       "  'Michigan State University',\n",
       "  'who',\n",
       "  \"b'develops\",\n",
       "  'new computational',\n",
       "  'methods',\n",
       "  'in',\n",
       "  'the',\n",
       "  'field',\n",
       "  'of',\n",
       "  'computational',\n",
       "  'biology',\n",
       "  'He',\n",
       "  \"b'has\",\n",
       "  'been ranked',\n",
       "  'as',\n",
       "  'the',\n",
       "  'number',\n",
       "  'one',\n",
       "  'most',\n",
       "  'influential',\n",
       "  'data',\n",
       "  'scientist',\n",
       "  'on',\n",
       "  'GitHub',\n",
       "  'by',\n",
       "  \"'\",\n",
       "  \"b'Analytics\",\n",
       "  'Vidhya',\n",
       "  'He',\n",
       "  'has',\n",
       "  'many',\n",
       "  'years',\n",
       "  'of',\n",
       "  'experience',\n",
       "  'with',\n",
       "  'coding',\n",
       "  'in',\n",
       "  'Python',\n",
       "  'and',\n",
       "  'he',\n",
       "  \"b'has\",\n",
       "  'conducted',\n",
       "  'several',\n",
       "  'seminars',\n",
       "  'on',\n",
       "  'the',\n",
       "  'practical',\n",
       "  'applications',\n",
       "  'of',\n",
       "  'data',\n",
       "  'science',\n",
       "  'and',\n",
       "  \"'\",\n",
       "  \"b'machine learning\",\n",
       "  'Talking',\n",
       "  'and',\n",
       "  'writing',\n",
       "  'about',\n",
       "  'data',\n",
       "  'science machine learning',\n",
       "  'and',\n",
       "  \"'\",\n",
       "  \"b'Python\",\n",
       "  'really',\n",
       "  'motivated',\n",
       "  'Sebastian',\n",
       "  'to',\n",
       "  'write',\n",
       "  'this',\n",
       "  'book',\n",
       "  'in',\n",
       "  'order',\n",
       "  'to',\n",
       "  'help',\n",
       "  'people',\n",
       "  'develop',\n",
       "  \"'\",\n",
       "  \"b'data-driven\",\n",
       "  'solutions',\n",
       "  'without',\n",
       "  'necessarily',\n",
       "  'needing',\n",
       "  'to',\n",
       "  'have',\n",
       "  'a',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  \"b'background\",\n",
       "  \"'\",\n",
       "  \"b'He\",\n",
       "  'has',\n",
       "  'also actively',\n",
       "  'contributed',\n",
       "  'to',\n",
       "  'open',\n",
       "  'source',\n",
       "  'projects',\n",
       "  'and',\n",
       "  'methods',\n",
       "  'that',\n",
       "  'he',\n",
       "  \"b'implemented\",\n",
       "  'which',\n",
       "  'are',\n",
       "  'now successfully',\n",
       "  'used',\n",
       "  'in',\n",
       "  'machine learning',\n",
       "  'competitions',\n",
       "  \"b'such\",\n",
       "  'as',\n",
       "  'Kaggle',\n",
       "  'In',\n",
       "  'his',\n",
       "  'free',\n",
       "  'time',\n",
       "  'he',\n",
       "  'works',\n",
       "  'on',\n",
       "  'models',\n",
       "  'for',\n",
       "  'sports predictions',\n",
       "  'and',\n",
       "  'if',\n",
       "  'he',\n",
       "  \"b'is\",\n",
       "  'not',\n",
       "  'in',\n",
       "  'front',\n",
       "  'of',\n",
       "  'the',\n",
       "  'computer',\n",
       "  'he',\n",
       "  'enjoys',\n",
       "  'playing',\n",
       "  'sports',\n",
       "  'b',\n",
       "  'I',\n",
       "  'would',\n",
       "  'like',\n",
       "  'to',\n",
       "  'thank',\n",
       "  'my',\n",
       "  'professors',\n",
       "  'Arun Ross',\n",
       "  'and',\n",
       "  'Pang-Ning Tan',\n",
       "  \"b'and\",\n",
       "  'many',\n",
       "  'others',\n",
       "  'who',\n",
       "  'inspired',\n",
       "  'me',\n",
       "  'and',\n",
       "  'kindled',\n",
       "  'my',\n",
       "  'great',\n",
       "  'interest',\n",
       "  'in',\n",
       "  \"b'pattern\",\n",
       "  'classification machine learning',\n",
       "  'and',\n",
       "  'data',\n",
       "  'mining b',\n",
       "  'I',\n",
       "  'would',\n",
       "  'like',\n",
       "  'to',\n",
       "  'take',\n",
       "  'this',\n",
       "  'opportunity',\n",
       "  'to',\n",
       "  'thank',\n",
       "  'the',\n",
       "  'great',\n",
       "  'Python',\n",
       "  \"'\",\n",
       "  \"b'community\",\n",
       "  'and',\n",
       "  'developers',\n",
       "  'of',\n",
       "  'open',\n",
       "  'source',\n",
       "  'packages',\n",
       "  'who',\n",
       "  'helped',\n",
       "  \"'\",\n",
       "  \"b'me\",\n",
       "  'create',\n",
       "  'the',\n",
       "  'perfect',\n",
       "  'environment',\n",
       "  'for',\n",
       "  'scientific',\n",
       "  'research',\n",
       "  'and',\n",
       "  \"'\",\n",
       "  \"b'data\",\n",
       "  'science b',\n",
       "  'A',\n",
       "  'special',\n",
       "  'thanks',\n",
       "  'goes',\n",
       "  'to',\n",
       "  'the',\n",
       "  'core',\n",
       "  'developers',\n",
       "  'of',\n",
       "  'scikit-learn',\n",
       "  'As',\n",
       "  'a',\n",
       "  \"b'contributor\",\n",
       "  'to',\n",
       "  'this',\n",
       "  'project',\n",
       "  'I',\n",
       "  'had',\n",
       "  'the',\n",
       "  'pleasure',\n",
       "  'to',\n",
       "  'work',\n",
       "  'with',\n",
       "  'great',\n",
       "  \"'\",\n",
       "  \"b'people\",\n",
       "  'who',\n",
       "  'are',\n",
       "  'not only very',\n",
       "  'knowledgeable',\n",
       "  'when',\n",
       "  'it',\n",
       "  'comes',\n",
       "  'to',\n",
       "  \"b'machine learning\",\n",
       "  'but',\n",
       "  'are',\n",
       "  'also',\n",
       "  'excellent',\n",
       "  'programmers',\n",
       "  \"b'Lastly\",\n",
       "  'I',\n",
       "  'want',\n",
       "  'to',\n",
       "  'thank',\n",
       "  'you',\n",
       "  'all',\n",
       "  'for',\n",
       "  'showing',\n",
       "  'an',\n",
       "  'interest',\n",
       "  'in',\n",
       "  'this',\n",
       "  \"book b'and\",\n",
       "  'I',\n",
       "  'sincerely',\n",
       "  'hope',\n",
       "  'that',\n",
       "  'I',\n",
       "  'can',\n",
       "  'pass',\n",
       "  'on',\n",
       "  'my',\n",
       "  'enthusiasm',\n",
       "  'to',\n",
       "  'join',\n",
       "  'the',\n",
       "  \"'\",\n",
       "  \"b'great\",\n",
       "  'Python',\n",
       "  'and',\n",
       "  'machine learning',\n",
       "  'communities',\n",
       "  \"b'About\",\n",
       "  'the',\n",
       "  'Reviewers',\n",
       "  \"'\",\n",
       "  \"b'Richard\",\n",
       "  'Dutton',\n",
       "  'started',\n",
       "  'programming',\n",
       "  'the',\n",
       "  'ZX Spectrum',\n",
       "  'when',\n",
       "  'he',\n",
       "  'was',\n",
       "  '8',\n",
       "  'years',\n",
       "  'old',\n",
       "  \"b'and\",\n",
       "  'his',\n",
       "  'obsession',\n",
       "  'carried',\n",
       "  'him',\n",
       "  'through',\n",
       "  'a',\n",
       "  'confusing',\n",
       "  'array',\n",
       "  'of',\n",
       "  'technologies',\n",
       "  'and',\n",
       "  'roles',\n",
       "  'in',\n",
       "  \"b'the\",\n",
       "  'fields',\n",
       "  'of',\n",
       "  'technology',\n",
       "  'and',\n",
       "  'finance',\n",
       "  \"'\",\n",
       "  \"b'He\",\n",
       "  'has',\n",
       "  'worked',\n",
       "  'with',\n",
       "  'Microsoft',\n",
       "  'and',\n",
       "  'as',\n",
       "  'a',\n",
       "  'Director',\n",
       "  'at',\n",
       "  'Barclays',\n",
       "  'his',\n",
       "  'current',\n",
       "  'obsession',\n",
       "  'is',\n",
       "  \"'\",\n",
       "  'b',\n",
       "  'a',\n",
       "  'mashup',\n",
       "  'of',\n",
       "  'Python',\n",
       "  'machine learning',\n",
       "  'and',\n",
       "  'block chain b',\n",
       "  'If',\n",
       "  'he',\n",
       "  \"'s\",\n",
       "  'not',\n",
       "  'in',\n",
       "  'front',\n",
       "  'of',\n",
       "  'a',\n",
       "  ...],\n",
       " 'tag': ['POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'NN',\n",
       "  'VBG',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'POS',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'TO',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'POS',\n",
       "  'JJ',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'CD',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNS',\n",
       "  'VBN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'MD',\n",
       "  'VB',\n",
       "  'VBN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'CC',\n",
       "  'VBN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'CC',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'JJ',\n",
       "  'VBN',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'NN',\n",
       "  'NNS',\n",
       "  'VBD',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'CC',\n",
       "  'NNS',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'VBZ',\n",
       "  'VBN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'POS',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'VBN',\n",
       "  'RB',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'VBN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'VBZ',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'NN',\n",
       "  'CC',\n",
       "  'NN',\n",
       "  'CC',\n",
       "  'VBN',\n",
       "  'CC',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'CC',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'CC',\n",
       "  'PRP$',\n",
       "  'NNS',\n",
       "  'CC',\n",
       "  'NNS',\n",
       "  'MD',\n",
       "  'VB',\n",
       "  'VBN',\n",
       "  'JJ',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NNS',\n",
       "  'POS',\n",
       "  'VBD',\n",
       "  'CC',\n",
       "  'VBN',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'VBN',\n",
       "  'RB',\n",
       "  'CC',\n",
       "  'RB',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'VBZ',\n",
       "  'VBN',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'POS',\n",
       "  'NNS',\n",
       "  'CC',\n",
       "  'NNS',\n",
       "  'VBN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'NNS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'MD',\n",
       "  'RB',\n",
       "  'VB',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'VBN',\n",
       "  'NNP',\n",
       "  'CD',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'CD',\n",
       "  'VBN',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'CD',\n",
       "  'NNP',\n",
       "  'JJ',\n",
       "  'CD',\n",
       "  'NN',\n",
       "  'NNS',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'JJ',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NNS',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNS',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'JJ',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNS',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NNS',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'JJ',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NNS',\n",
       "  'POS',\n",
       "  'JJ',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'JJ',\n",
       "  'VBP',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'VBG',\n",
       "  'TO',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'CD',\n",
       "  'NN',\n",
       "  'CD',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'NNS',\n",
       "  'VBP',\n",
       "  'VBN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'DT',\n",
       "  'VBZ',\n",
       "  'RB',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'CD',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'PRP',\n",
       "  'VBP',\n",
       "  'NNS',\n",
       "  'VBD',\n",
       "  'VBN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'RB',\n",
       "  'JJS',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'MD',\n",
       "  'RB',\n",
       "  'VB',\n",
       "  'VBN',\n",
       "  'IN',\n",
       "  'NNS',\n",
       "  'CC',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'VBZ',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'CC',\n",
       "  'PRP',\n",
       "  'VBZ',\n",
       "  'RB',\n",
       "  'JJ',\n",
       "  'IN',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'VBD',\n",
       "  'NNS',\n",
       "  'TO',\n",
       "  'RB',\n",
       "  'VB',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'PRP',\n",
       "  'VBP',\n",
       "  'NNS',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'CC',\n",
       "  'VB',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'WDT',\n",
       "  'VBP',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'VBZ',\n",
       "  'TO',\n",
       "  'DT',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'PRP',\n",
       "  'VBP',\n",
       "  'IN',\n",
       "  'PRP$',\n",
       "  'NNS',\n",
       "  'PRP',\n",
       "  'VBP',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'TO',\n",
       "  'NN',\n",
       "  'JJS',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'IN',\n",
       "  'RB',\n",
       "  'VBG',\n",
       "  'PRP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'PRP',\n",
       "  'RB',\n",
       "  'VBZ',\n",
       "  'PRP',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'WP',\n",
       "  'VBZ',\n",
       "  'NNP',\n",
       "  'CC',\n",
       "  'WRB',\n",
       "  'VBZ',\n",
       "  'PRP',\n",
       "  'VB',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'MD',\n",
       "  'PRP',\n",
       "  'VBP',\n",
       "  'JJ',\n",
       "  'NNP',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'PRP$',\n",
       "  'NN',\n",
       "  'CC',\n",
       "  'RB',\n",
       "  'VB',\n",
       "  'RP',\n",
       "  'WP',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'PRP$',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'CC',\n",
       "  'JJR',\n",
       "  'MD',\n",
       "  'VB',\n",
       "  'VBN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'VBN',\n",
       "  'IN',\n",
       "  'PRP$',\n",
       "  'JJ',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'CC',\n",
       "  'NN',\n",
       "  'JJ',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'RB',\n",
       "  'IN',\n",
       "  'VBG',\n",
       "  'PRP$',\n",
       "  'NN',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'VBZ',\n",
       "  'RB',\n",
       "  'VBD',\n",
       "  'PRP$',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'TO',\n",
       "  'DT',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'POS',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'NNP',\n",
       "  'VBZ',\n",
       "  'VBN',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'NN',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'CC',\n",
       "  'NNS',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'PRP',\n",
       "  'VBZ',\n",
       "  'RB',\n",
       "  'VBN',\n",
       "  'NN',\n",
       "  'VBD',\n",
       "  'TO',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'NNS',\n",
       "  'JJ',\n",
       "  'IN',\n",
       "  'WDT',\n",
       "  'VBP',\n",
       "  'RB',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'NN',\n",
       "  'VBG',\n",
       "  'TO',\n",
       "  'PRP$',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'PRP',\n",
       "  'VBP',\n",
       "  'JJ',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'MD',\n",
       "  'VB',\n",
       "  'JJ',\n",
       "  'TO',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'NNS',\n",
       "  'PRP',\n",
       "  'RB',\n",
       "  'VBP',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'TO',\n",
       "  'NN',\n",
       "  'VBG',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'DT',\n",
       "  'JJR',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'RBR',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'CC',\n",
       "  'NNP',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'JJ',\n",
       "  'NNP',\n",
       "  'VBZ',\n",
       "  'DT',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'WP',\n",
       "  'VBZ',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'PRP',\n",
       "  'RB',\n",
       "  'VBN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'CD',\n",
       "  'JJS',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'IN',\n",
       "  'POS',\n",
       "  'NNS',\n",
       "  'NNP',\n",
       "  'PRP',\n",
       "  'VBZ',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'CC',\n",
       "  'PRP',\n",
       "  'RB',\n",
       "  'VBN',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'NNS',\n",
       "  'NN',\n",
       "  'CC',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'VBG',\n",
       "  'CC',\n",
       "  'VBG',\n",
       "  'IN',\n",
       "  'NNS',\n",
       "  'NN',\n",
       "  'CC',\n",
       "  'POS',\n",
       "  'NNS',\n",
       "  'RB',\n",
       "  'VBD',\n",
       "  'JJ',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'NN',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'NNS',\n",
       "  'VB',\n",
       "  'POS',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'RB',\n",
       "  'VBG',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'VBG',\n",
       "  'NN',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'VBZ',\n",
       "  'RB',\n",
       "  'VBD',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'NN',\n",
       "  'NNS',\n",
       "  'CC',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'PRP',\n",
       "  'VBN',\n",
       "  'WDT',\n",
       "  'VBP',\n",
       "  'RB',\n",
       "  'VBN',\n",
       "  'IN',\n",
       "  'NN',\n",
       "  'NNS',\n",
       "  'JJ',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'IN',\n",
       "  'PRP$',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'PRP',\n",
       "  'VBZ',\n",
       "  'IN',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'NNS',\n",
       "  'CC',\n",
       "  'IN',\n",
       "  'PRP',\n",
       "  'NN',\n",
       "  'RB',\n",
       "  'IN',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'PRP',\n",
       "  'VBZ',\n",
       "  'VBG',\n",
       "  'NNS',\n",
       "  'NN',\n",
       "  'PRP',\n",
       "  'MD',\n",
       "  'VB',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'PRP$',\n",
       "  'NNS',\n",
       "  'NNP',\n",
       "  'CC',\n",
       "  'NNP',\n",
       "  'VBP',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'WP',\n",
       "  'VBD',\n",
       "  'PRP',\n",
       "  'CC',\n",
       "  'VBD',\n",
       "  'PRP$',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'CC',\n",
       "  'NNS',\n",
       "  'NN',\n",
       "  'PRP',\n",
       "  'MD',\n",
       "  'VB',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'DT',\n",
       "  'JJ',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'CC',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'NNS',\n",
       "  'WP',\n",
       "  'VBD',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'VBP',\n",
       "  'DT',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'CC',\n",
       "  'POS',\n",
       "  'NNS',\n",
       "  'NN',\n",
       "  'DT',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'VBZ',\n",
       "  'TO',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'TO',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'PRP',\n",
       "  'VBD',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'WP',\n",
       "  'VBP',\n",
       "  'RB',\n",
       "  'JJ',\n",
       "  'WRB',\n",
       "  'PRP',\n",
       "  'VBZ',\n",
       "  'TO',\n",
       "  'NN',\n",
       "  'CC',\n",
       "  'VBP',\n",
       "  'RB',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'RB',\n",
       "  'PRP',\n",
       "  'VBP',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'PRP',\n",
       "  'DT',\n",
       "  'IN',\n",
       "  'VBG',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'PRP',\n",
       "  'RB',\n",
       "  'VBP',\n",
       "  'IN',\n",
       "  'PRP',\n",
       "  'MD',\n",
       "  'VB',\n",
       "  'IN',\n",
       "  'PRP$',\n",
       "  'NN',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'DT',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'CC',\n",
       "  'NN',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NNP',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'VBD',\n",
       "  'VBG',\n",
       "  'DT',\n",
       "  'NNP',\n",
       "  'WRB',\n",
       "  'PRP',\n",
       "  'VBD',\n",
       "  'CD',\n",
       "  'NNS',\n",
       "  'JJ',\n",
       "  'VB',\n",
       "  'PRP$',\n",
       "  'NN',\n",
       "  'VBD',\n",
       "  'PRP',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'NNS',\n",
       "  'CC',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'IN',\n",
       "  'NN',\n",
       "  'CC',\n",
       "  'NN',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'VBZ',\n",
       "  'VBN',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'CC',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NNP',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'PRP$',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'VBZ',\n",
       "  'POS',\n",
       "  'NN',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'NN',\n",
       "  'CC',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'PRP',\n",
       "  'VBZ',\n",
       "  'RB',\n",
       "  'IN',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  ...]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_word = pd.DataFrame(words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Python</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'Unlock deeper</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104721</th>\n",
       "      <td>for</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104722</th>\n",
       "      <td>information</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104723</th>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104724</th>\n",
       "      <td>our</td>\n",
       "      <td>PRP$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104725</th>\n",
       "      <td>titles</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104726 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    word   tag\n",
       "0                      '   POS\n",
       "1               b'Python    NN\n",
       "2       Machine Learning   NNP\n",
       "3                      '   POS\n",
       "4        b'Unlock deeper    NN\n",
       "...                  ...   ...\n",
       "104721               for    IN\n",
       "104722       information    NN\n",
       "104723                on    IN\n",
       "104724               our  PRP$\n",
       "104725            titles   NNS\n",
       "\n",
       "[104726 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POS', 'NN', 'NNP', 'NNS', 'IN', 'VBG', 'DT', 'JJ', 'TO', 'CD',\n",
       "       'VBN', 'MD', 'VB', 'CC', 'VBD', 'VBZ', 'RB', 'PRP$', 'VBP', 'PRP',\n",
       "       'JJS', 'WDT', 'WP', 'WRB', 'RP', 'JJR', 'RBR', 'RBS', 'NNPS',\n",
       "       'PDT', 'EX', 'FW', 'SYM', 'UH', 'WP$'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_word.tag.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_word.drop_duplicates(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Python</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'Unlock deeper</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>insights</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104699</th>\n",
       "      <td>code organization</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104701</th>\n",
       "      <td>b'putting</td>\n",
       "      <td>VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104709</th>\n",
       "      <td>Covers</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104710</th>\n",
       "      <td>classification regression feature b'engineering</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104714</th>\n",
       "      <td>guided</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17445 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   word  tag\n",
       "0                                                     '  POS\n",
       "1                                              b'Python   NN\n",
       "2                                      Machine Learning  NNP\n",
       "4                                       b'Unlock deeper   NN\n",
       "5                                              insights  NNS\n",
       "...                                                 ...  ...\n",
       "104699                                code organization   NN\n",
       "104701                                        b'putting  VBG\n",
       "104709                                           Covers  NNP\n",
       "104710  classification regression feature b'engineering   NN\n",
       "104714                                           guided   JJ\n",
       "\n",
       "[17445 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346                        most\n",
       "1048                        web\n",
       "2664                     forest\n",
       "2989                     invest\n",
       "3623                 b'toughest\n",
       "5084                       best\n",
       "7706                      least\n",
       "7720                     b'best\n",
       "8768                   earliest\n",
       "9115                    hottest\n",
       "11716                   highest\n",
       "11741                   largest\n",
       "12532                   biggest\n",
       "16584                    y_test\n",
       "16685                    X_test\n",
       "16802                      Most\n",
       "17454           b'X_test y_test\n",
       "17472               label='test\n",
       "20746                   closest\n",
       "25159                  b'forest\n",
       "25406         classifier=forest\n",
       "25796                   nearest\n",
       "27343                   easiest\n",
       "29392                    b'test\n",
       "30197                  smallest\n",
       "31530                     'Test\n",
       "31534             y_test b'Test\n",
       "33459                    b'Test\n",
       "34910                 b'largest\n",
       "35912                 loc='best\n",
       "44141      X_test y_test b'Test\n",
       "48557             y_true=y_test\n",
       "54329            splitter='best\n",
       "54342                     'best\n",
       "54591                     'Best\n",
       "54594                      Best\n",
       "55345                 tree_test\n",
       "55354                train/test\n",
       "55428                  bag_test\n",
       "57310                  ada_test\n",
       "58558                     'test\n",
       "61181                    oldest\n",
       "61183                  simplest\n",
       "62036             X_test y_test\n",
       "63665                      dest\n",
       "63838                    latest\n",
       "65219                  b'latest\n",
       "68648                   tiniest\n",
       "73605               label='Test\n",
       "78234                    lowest\n",
       "78699                  farthest\n",
       "80245                 b'nearest\n",
       "82416    interpolation='nearest\n",
       "84333                b'interest\n",
       "94895                  cheapest\n",
       "Name: word, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_word[data_word['tag']== 'JJS']['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_notes = ['POS', 'NN', 'VBP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining starting and ending protocols\n",
    "def generate_exams(keyword, type_exam, no_questions):\n",
    "    flag = True\n",
    "    questions = []\n",
    "    answers = []\n",
    "    no_questions_generated = 0\n",
    "    \n",
    "    while flag:\n",
    "        if no_questions_generated == no_questions:\n",
    "            flag = False\n",
    "        if type_exam == 'multiple_coice':\n",
    "            questions.append(1)\n",
    "        \n",
    "        elif type_exam == 'Match item':\n",
    "            questions.append(1)\n",
    "        \n",
    "        elif type_exam == 'True':\n",
    "            questions.append(1)\n",
    "            \n",
    "        elif type_exam == 'fill the blanks':\n",
    "            questions.append(1)\n",
    "            \n",
    "        elif type_exam == 'Short Note':\n",
    "            type_word_selected = random.choice(short_notes)\n",
    "            words_df_len = data_word[data_word['tag']== type_word_selected]['word'].shape[0]\n",
    "            index_choosen = random.randint(0, words_df_len-1)\n",
    "            Qn_generated= data_word[data_word['tag']== type_word_selected]['word'].iloc[index_choosen]\n",
    "            \n",
    "            questions.append(Qn_generated)\n",
    "            \n",
    "        elif type_exam =='Essay':\n",
    "            questions.append(1)\n",
    "\n",
    "        else:\n",
    "            questions.append(1)\n",
    "            \n",
    "        user_input = keyword\n",
    "        user_input = user_input.lower()\n",
    "\n",
    "#         sent_tokens.append(user_input)\n",
    "#         word_tokens = word_tokens +nltk.word_tokenize(user_input)\n",
    "#         final_words = list(set(word_tokens))\n",
    "#         print('BOT : ', end='')\n",
    "#         result = response(user_input)\n",
    "#         print(result)\n",
    "#         sent_tokens.remove(user_input)\n",
    "        no_questions_generated +=1\n",
    "    return questions, answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = generate_exams(keyword = 'called', type_exam= 'Short Note', no_questions = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"b'pickle approach\",\n",
       " \"b'vector dot product\",\n",
       " \"random guessing b'and classification\",\n",
       " 'briefly',\n",
       " 'right plot',\n",
       " \"b'Scikit-learn\"]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "956876856fac9874005287f97a8429fd104b069a0c68cff37c79f7bf80e86b22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
